{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASEWA-ResNet50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASEWA Project - ResNet50\n",
        "Using the pretrained ResNet50 to perform Style Transfer\n",
        "---\n",
        "Janina Klarmann, Laura KÃ¼hl"
      ],
      "metadata": {
        "id": "EI-jIkECyr_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "zFfhaSfkzDqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIkSEliIyrL_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (10,10)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import functools\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display as display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import models \n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "metadata": {
        "id": "F5CehPTqzU6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.compat.v1.disable_eager_execution()\n",
        "# print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "metadata": {
        "id": "i_tvPpO8zYsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images"
      ],
      "metadata": {
        "id": "9IizqYRwzUe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
      ],
      "metadata": {
        "id": "WOhMkEWCzfwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers"
      ],
      "metadata": {
        "id": "UFx9CGIIzo6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Layers"
      ],
      "metadata": {
        "id": "ZpnEyPy2z1QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Content layer \n",
        "content_layers_resnet50 = ['conv1_conv']\n",
        "\n",
        "# # # Style layer\n",
        "style_layers_resnet50 = ['conv2_block2_1_bn','conv3_block2_3_bn','conv3_block3_3_conv','conv4_block2_2_bn'#,'conv1'\n",
        "               ]\n",
        "\n",
        "num_content_layers = len(content_layers_resnet50)\n",
        "num_style_layers = len(style_layers_resnet50)"
      ],
      "metadata": {
        "id": "h98J6zwyzqdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers for Layer Selection Analysis"
      ],
      "metadata": {
        "id": "jjNfptwPz5LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Last layers of blocks and content\n",
        "# style_layers_resnet50 = ['conv2_block1_3_conv','conv2_block2_3_conv','conv2_block3_3_conv','conv3_block1_3_conv','conv3_block2_3_conv','conv3_block3_3_conv', 'conv4_block1_3_conv','conv4_block2_3_conv','conv4_block3_3_conv', 'conv5_block1_3_conv','conv5_block2_3_conv','conv5_block3_3_conv']\n",
        "# content_layers_resnet50 = ['conv5_block3_out']\n",
        "\n",
        "## Last style, first content\n",
        "# style_layers_resnet50 = ['conv2_block1_3_conv','conv2_block2_3_conv','conv2_block3_3_conv','conv3_block1_3_conv','conv3_block2_3_conv','conv3_block3_3_conv', 'conv4_block1_3_conv','conv4_block2_3_conv','conv4_block3_3_conv', 'conv5_block1_3_conv','conv5_block2_3_conv','conv5_block3_3_conv']\n",
        "# content_layers_resnet50 = ['conv1_conv']\n",
        "\n",
        "## First style, first content\n",
        "# style_layers_resnet50 = ['conv2_block1_1_conv','conv3_block1_1_conv','conv4_block1_1_conv','conv5_block1_1_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## Second Style, first content\n",
        "# style_layers_resnet50 = ['conv2_block1_2_conv','conv3_block1_2_conv','conv4_block1_2_conv','conv5_block1_2_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## second block, first layers\n",
        "# style_layers_resnet50 = ['conv2_block2_1_conv','conv3_block2_1_conv','conv4_block2_1_conv','conv5_block2_1_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## third block, first style, first content\n",
        "# style_layers_resnet50 = ['conv2_block3_1_conv','conv3_block3_1_conv','conv4_block3_1_conv','conv5_block3_1_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## second block, second style, first content\n",
        "# style_layers_resnet50 = ['conv2_block2_2_conv','conv3_block2_2_conv','conv4_block2_2_conv','conv5_block2_2_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## second block, third style, first content\n",
        "# style_layers_resnet50 = ['conv2_block2_3_conv','conv3_block2_3_conv','conv4_block2_3_conv','conv5_block2_3_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## third block, second style, first content\n",
        "# style_layers_resnet50 = ['conv2_block3_2_conv','conv3_block3_2_conv','conv4_block3_2_conv','conv5_block3_2_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']\n",
        "\n",
        "## third block, third style, first content\n",
        "# style_layers_resnet50 = ['conv2_block3_3_conv','conv3_block3_3_conv','conv4_block3_3_conv','conv5_block3_3_conv']\n",
        "# content_layer_resnet50 = ['conv1_conv']"
      ],
      "metadata": {
        "id": "fcnNKdie0EaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Style Transfer "
      ],
      "metadata": {
        "id": "-UItUbxV0SXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "dkSgf-5P0YqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet50_Model(style_layers, content_layers):\n",
        "  \"\"\" Creates our model with access to intermediate layers. \n",
        "  \n",
        "  This function will load the ResNet50 model and access the intermediate layers. \n",
        "  These layers will then be used to create a new model that will take input image\n",
        "  and return the outputs from these intermediate layers from the ResNet50 model. \n",
        "  \n",
        "  Returns:\n",
        "    returns a keras model that takes image inputs and outputs the style and \n",
        "      content intermediate layers. \n",
        "  \"\"\"\n",
        "  # Load our model. We load pretrained ResNet50, trained on imagenet data\n",
        "  resnet50 = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\n",
        "  resnet50.trainable = False\n",
        "  # Get output layers corresponding to style and content layers \n",
        "  style_outputs = [resnet50.get_layer(name).output for name in style_layers]\n",
        "  content_outputs = [resnet50.get_layer(name).output for name in content_layers]\n",
        "  model_outputs = style_outputs + content_outputs\n",
        "  # Build model \n",
        "  return models.Model(resnet50.input, model_outputs)"
      ],
      "metadata": {
        "id": "IxhyKaYS0X86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleExtractionModelResNet50(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleExtractionModelResNet50, self).__init__()\n",
        "    self.resnet50 = ResNet50_Model(style_layers, content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.content_path = content_path\n",
        "    self.style_path = style_path\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.resnet50.trainable = False \n",
        "\n",
        "  def gram_matrix(self, input_tensor):\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "  def load_img(self, path_to_img):\n",
        "    max_dim = 512\n",
        "    img = Image.open(path_to_img)\n",
        "    long = max(img.size)\n",
        "    scale = max_dim/long\n",
        "    img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
        "    \n",
        "    # img = kp_image.img_to_array(img)\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    \n",
        "    # We need to broadcast the image array such that it has a batch dimension \n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "  def load_and_process_img(self, path_to_img):\n",
        "    img = self.load_img(path_to_img)\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "  def call(self):\n",
        "    # return {'content': content_dict, 'style': style_dict}\n",
        "    content_image = self.load_and_process_img(self.content_path)\n",
        "    style_image = self.load_and_process_img(self.style_path)\n",
        "    \n",
        "    # batch compute content and style features\n",
        "    style_outputs = self.resnet50(style_image)\n",
        "    content_outputs = self.resnet50(content_image)\n",
        "    \n",
        "    # Get the style and content feature representations from our model  \n",
        "    style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
        "    content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
        "    return style_features, content_features"
      ],
      "metadata": {
        "id": "4lKXvwzg0fUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleTrainingModelResNet50(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers, num_iterations=10):\n",
        "    super(StyleTrainingModelResNet50, self).__init__()\n",
        "    self.ExtractionModel = StyleExtractionModelResNet50(style_layers, content_layers)\n",
        "    self.model = ResNet50_Model(style_layers, content_layers)\n",
        "    self.opt = tf.compat.v1.train.AdamOptimizer(learning_rate=20, beta1=0.99, epsilon=1e-1)\n",
        "    self.style_weight = 1e7\n",
        "    self.content_weight = 1\n",
        "    self.loss_weights = (self.style_weight, self.content_weight)\n",
        "    self.style_features, self.content_features = self.ExtractionModel.call()\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.num_content_layers = len(content_layers)\n",
        "    self.gram_style_features = [self.ExtractionModel.gram_matrix(style_feature) for style_feature in self.style_features]\n",
        "    self.init_image = tf.Variable(self.ExtractionModel.load_and_process_img(content_path), dtype=tf.float32)\n",
        "    self.best_loss, self.best_img = float('inf'), None\n",
        "    self.cfg = {\n",
        "        'model': self.model,\n",
        "        'loss_weights': self.loss_weights,\n",
        "        'init_image': self.init_image,\n",
        "        'gram_style_features': self.gram_style_features,\n",
        "        'content_features': self.content_features\n",
        "    }\n",
        "    # For displaying\n",
        "    self.num_rows = 2\n",
        "    self.num_cols = 5\n",
        "    self.num_iterations = num_iterations\n",
        "    self.display_interval = self.num_iterations/(self.num_rows*self.num_cols)\n",
        "    self.start_time = time.time()\n",
        "    self.global_start = time.time()\n",
        "    self.norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    self.min_vals = -self.norm_means\n",
        "    self.max_vals = 255 - self.norm_means \n",
        "\n",
        "  def deprocess_img(self, processed_img):\n",
        "    x = processed_img.copy()\n",
        "    if len(x.shape) == 4:\n",
        "      x = np.squeeze(x, 0)\n",
        "    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
        "                              \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
        "    if len(x.shape) != 3:\n",
        "      raise ValueError(\"Invalid input to deprocessing image\")\n",
        "    \n",
        "    # perform the inverse of the preprocessiing step\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    x = x[:, :, ::-1]\n",
        "\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "  def get_style_loss(self, base_style, gram_target):\n",
        "    \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
        "    # height, width, num filters of each layer\n",
        "    # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
        "    height, width, channels = base_style.get_shape().as_list()\n",
        "    gram_style = self.ExtractionModel.gram_matrix(base_style)\n",
        "    \n",
        "    return tf.reduce_mean(tf.square(gram_style - gram_target))\n",
        "  \n",
        "  def get_content_loss(self, base_content, target):\n",
        "    return tf.reduce_mean(tf.square(base_content - target))\n",
        "\n",
        "  def compute_loss(self, model, loss_weights, init_image, gram_style_features, content_features):\n",
        "    \"\"\"This function will compute the loss total loss.\n",
        "    \n",
        "    Arguments:\n",
        "      model: The model that will give us access to the intermediate layers\n",
        "      loss_weights: The weights of each contribution of each loss function. \n",
        "        (style weight, content weight, and total variation weight)\n",
        "      init_image: Our initial base image. This image is what we are updating with \n",
        "        our optimization process. We apply the gradients wrt the loss we are \n",
        "        calculating to this image.\n",
        "      gram_style_features: Precomputed gram matrices corresponding to the \n",
        "        defined style layers of interest.\n",
        "      content_features: Precomputed outputs from defined content layers of \n",
        "        interest.\n",
        "        \n",
        "    Returns:\n",
        "      returns the total loss, style loss, content loss, and total variational loss\n",
        "    \"\"\"\n",
        "    style_weight, content_weight = loss_weights\n",
        "    \n",
        "    # Feed our init image through our model. This will give us the content and \n",
        "    # style representations at our desired layers. \n",
        "    model_outputs = model(init_image)\n",
        "    \n",
        "    style_output_features = model_outputs[:num_style_layers]\n",
        "    content_output_features = model_outputs[num_style_layers:]\n",
        "    \n",
        "    style_score = 0\n",
        "    content_score = 0\n",
        "\n",
        "    # Accumulate style losses from all layers\n",
        "    # Equally weight each contribution of each loss layer\n",
        "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
        "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
        "      style_score += weight_per_style_layer * self.get_style_loss(comb_style[0], target_style)\n",
        "      \n",
        "    # Accumulate content losses from all layers \n",
        "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
        "    for target_content, comb_content in zip(content_features, content_output_features):\n",
        "      content_score += weight_per_content_layer* self.get_content_loss(comb_content[0], target_content)\n",
        "    \n",
        "    style_score *= style_weight\n",
        "    content_score *= content_weight\n",
        "\n",
        "    # Get total loss\n",
        "    loss = style_score + content_score \n",
        "    return loss, style_score, content_score\n",
        "\n",
        "  def compute_grads(self, cfg):\n",
        "    with tf.GradientTape() as tape: \n",
        "      all_loss = self.compute_loss(**cfg)\n",
        "    # Compute gradients wrt input image\n",
        "    total_loss = all_loss[0]\n",
        "    return tape.gradient(total_loss, self.cfg['init_image']), all_loss\n",
        "\n",
        "  tf.config.run_functions_eagerly(True)\n",
        "  @tf.function()\n",
        "  def train_step(self):\n",
        "    # tf.compat.v1.disable_eager_execution()\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    imgs = []\n",
        "    for i in range(self.num_iterations):\n",
        "      grads, all_loss = self.compute_grads(self.cfg)\n",
        "      loss, style_score, content_score = all_loss\n",
        "      self.opt.apply_gradients([(grads, self.init_image)])\n",
        "      clipped = tf.clip_by_value(self.init_image, self.min_vals, self.max_vals)\n",
        "      self.init_image.assign(clipped)\n",
        "      end_time = time.time() \n",
        "      \n",
        "      print(\". \", end=\"\") # Fo tracking progress\n",
        "      \n",
        "      if loss < self.best_loss:\n",
        "        # Update best loss and best image from total loss. \n",
        "        self.best_loss = loss\n",
        "        best_img = self.deprocess_img(self.init_image.numpy())\n",
        "\n",
        "      if i % self.display_interval== 0:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Use the .numpy() method to get the concrete numpy array\n",
        "        plot_img = self.init_image.numpy()\n",
        "        plot_img = self.deprocess_img(plot_img)\n",
        "        imgs.append(plot_img)\n",
        "        display.clear_output(wait=True)\n",
        "        display.display_png(Image.fromarray(plot_img))\n",
        "        print('Iteration: {}'.format(i))        \n",
        "        print('Total loss: {:.4e}, ' \n",
        "              'style loss: {:.4e}, '\n",
        "              'content loss: {:.4e}, '\n",
        "              'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n",
        "    print('Total time: {:.4f}s'.format(time.time() - self.global_start))\n",
        "    display.clear_output(wait=True)\n",
        "    plt.figure(figsize=(14,4))\n",
        "    for i,img in enumerate(imgs):\n",
        "        plt.subplot(self.num_rows,self.num_cols,i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    return best_img, self.best_loss\n",
        "    # tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "id": "ehTCnAoC0j1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.compat.v1.disable_eager_execution()\n",
        "# print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "metadata": {
        "id": "298WFtgl1CBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Style Transfer"
      ],
      "metadata": {
        "id": "7_oV1OFI1HV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyModelResNet50 = StyleTrainingModelResNet50(style_layers_resnet50, content_layers_resnet50, num_iterations=100)"
      ],
      "metadata": {
        "id": "WBxt2syD1K-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best, best_loss = MyModelResNet50.train_step()"
      ],
      "metadata": {
        "id": "OFTc9mLr1L0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bestImage = Image.fromarray(best)\n",
        "bestImage"
      ],
      "metadata": {
        "id": "Zp3LtfEP1PCV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}