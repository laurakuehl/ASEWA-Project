{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport math\n\nimport PIL.Image\nimport IPython.display as display\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport time\nimport functools\n# for looking at files\nimport glob\nimport tqdm\nimport datetime\n\nfrom keras.models import load_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:20:57.185811Z","iopub.execute_input":"2022-04-10T12:20:57.186145Z","iopub.status.idle":"2022-04-10T12:21:04.500190Z","shell.execute_reply.started":"2022-04-10T12:20:57.186112Z","shell.execute_reply":"2022-04-10T12:21:04.499391Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"style_to_number = {\n    'cubism': 0,\n    'expressionism' : 1,\n    'romanticism' : 2\n}\n\ndef load_images(folder):\n        '''Load images and create corresponding numerical labels for the classes\n    Input: \n            path: a path\n            \n    Output:\n            images: a list of arrays \n            labels: a list of numerical labels'''\n    \n    \n    # load all path for the images, respectively to their epoch\n    path = '../input/art-movements/dataset/' + folder\n    \n    cubism_paths = glob.glob(path + '/cubism/*')\n    expressionism_paths = glob.glob(path + '/expressionism/*')\n    romanticism_paths = glob.glob(path + '/romanticism/*')\n    \n    combined_paths = [cubism_paths, expressionism_paths, romanticism_paths]\n    \n    images = []\n    labels = []\n\n    # load images and create art-style-corresponding label list for them\n    for i, art_style in enumerate(combined_paths):\n        for image_path in art_style:\n            image = np.asarray(tf.keras.preprocessing.image.load_img(image_path))     \n            images.append(image)\n            labels.append(i)\n\n    return images, labels\n\n\ndef resize_images(images):\n    '''Images get resized into a uniform size'''\n    \n    return [tf.image.resize(image, [128,128]) for image in images]\n\n\ndef crop_images(images):\n    '''Crop the image in the biggest possible square\n        \n            Input: Array of images\n            \n            Output: Array if square image'''\n    \n    cropped = []\n    \n    for image in images:\n        shape = np.min(image.shape[:-1])\n        cropped_image = tf.image.resize_with_crop_or_pad(image, shape, shape)\n        cropped.append(cropped_image)\n\n    return cropped\n\n\n# we did not use it, but it could be used to create more examples for training the network. \n# It is questionable if the style may gets distorted too much\ndef random_crop(images):\n    '''Randomly crop all images into a uiform size'''\n    return [tf.image.random_crop(image, size=[128, 128, 3]) for image in images]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:04.501948Z","iopub.execute_input":"2022-04-10T12:21:04.502171Z","iopub.status.idle":"2022-04-10T12:21:04.513896Z","shell.execute_reply.started":"2022-04-10T12:21:04.502142Z","shell.execute_reply":"2022-04-10T12:21:04.513260Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# create arrays of labels ad images\ntrain_images, train_labels = load_images(folder='train')\ntest_images, test_labels = load_images(folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:04.515900Z","iopub.execute_input":"2022-04-10T12:21:04.516216Z","iopub.status.idle":"2022-04-10T12:21:45.245249Z","shell.execute_reply.started":"2022-04-10T12:21:04.516156Z","shell.execute_reply":"2022-04-10T12:21:45.244275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# crop and resize images into a uniform shape to be able to create a tensorflow dataset\ntrain_images_cropped = crop_images(train_images)\ntrain_images_resized = resize_images(train_images_cropped)\ntest_images_cropped = crop_images(test_images)\ntest_images_resized = resize_images(test_images_cropped)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:45.247164Z","iopub.execute_input":"2022-04-10T12:21:45.247469Z","iopub.status.idle":"2022-04-10T12:21:50.501105Z","shell.execute_reply.started":"2022-04-10T12:21:45.247435Z","shell.execute_reply":"2022-04-10T12:21:50.500149Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Create Datasets ","metadata":{}},{"cell_type":"code","source":"# Generator do merge the images and labels for train and test-datasets\ndef train_data_gen():\n    for i, image in enumerate(train_images):\n        yield image, train_labels[i]\n\ndef test_data_gen():\n    for i, image in enumerate(test_images):\n        yield image, test_labels[i]\n\n\n# creste a tf datasets from the loaded images and the labels\ntrain_ds = tf.data.Dataset.from_generator(train_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n                                                             tf.TensorSpec(shape=(), dtype=tf.int32))\n                                                             )\n\ntest_ds = tf.data.Dataset.from_generator(test_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n                                                             tf.TensorSpec(shape=(), dtype=tf.int32))\n                                                             )","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:11:24.826220Z","iopub.execute_input":"2022-04-10T13:11:24.826902Z","iopub.status.idle":"2022-04-10T13:11:24.871079Z","shell.execute_reply.started":"2022-04-10T13:11:24.826846Z","shell.execute_reply":"2022-04-10T13:11:24.870396Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"#### Plottting some Images","metadata":{}},{"cell_type":"code","source":"plt.imshow(2*(train_images[1]/256)-1) ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:10:19.441347Z","iopub.execute_input":"2022-04-10T13:10:19.441687Z","iopub.status.idle":"2022-04-10T13:10:19.862044Z","shell.execute_reply.started":"2022-04-10T13:10:19.441649Z","shell.execute_reply":"2022-04-10T13:10:19.861131Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_images_cropped[1]) ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:08:55.231384Z","iopub.execute_input":"2022-04-10T13:08:55.231693Z","iopub.status.idle":"2022-04-10T13:08:55.495145Z","shell.execute_reply.started":"2022-04-10T13:08:55.231660Z","shell.execute_reply":"2022-04-10T13:08:55.494308Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"plt.imshow(2*(train_images_resized[1]/256)-1) ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:08:46.410814Z","iopub.execute_input":"2022-04-10T13:08:46.411170Z","iopub.status.idle":"2022-04-10T13:08:46.603366Z","shell.execute_reply.started":"2022-04-10T13:08:46.411136Z","shell.execute_reply":"2022-04-10T13:08:46.602478Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"# create the generator we \ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255, \n    #rotation_range=40,\n    brightness_range=(0.5,1.5),\n    #width_shift_range=0.15,\n    #height_shift_range=0.15,\n    shear_range=5,\n    horizontal_flip=True,\n    vertical_flip = True,\n    zoom_range = [0.7, 1]\n)\n\n# create an iterator using datagen.flow\ntrain_images_resized = np.asarray(train_images_resized)\ntrain_labels = np.asarray(train_labels)\ntrain_generator = datagen.flow(train_images_resized, train_labels, batch_size=64)\n\n\ndef generator(num_batches):\n    for i, train_tuple in enumerate(train_generator):\n        yield train_tuple\n        if i >= num_batches:\n            return","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:50.591178Z","iopub.execute_input":"2022-04-10T12:21:50.591530Z","iopub.status.idle":"2022-04-10T12:21:50.670186Z","shell.execute_reply.started":"2022-04-10T12:21:50.591487Z","shell.execute_reply":"2022-04-10T12:21:50.669291Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Plott Augmented Data","metadata":{}},{"cell_type":"code","source":"###Plotting Augmented Images\n\n# plot images of first batch\nfig, ax = plt.subplots(4,8,figsize=(20,10))\nfig.tight_layout()\nax = ax.flatten()\nfor img_b, label_b in train_generator:\n    for i in range(32):   \n        img = img_b[i]\n        l = label_b[i]\n        \n        ax[i].imshow(img)  \n        ax[i].set_title((img.shape))\n        ax[i].axis(\"off\")\n\n    break   \n\nfig, ax = plt.subplots(4,8,figsize=(20,10))\nfig.tight_layout()\nax = ax.flatten()\nfor img_b, label_b in train_generator:\n    for i in range(32):   \n        img = img_b[i]\n        l = label_b[i]\n        ax[i].imshow(img)  \n        ax[i].set_title((img.shape))\n        ax[i].axis(\"off\")\n\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-05T17:30:45.833742Z","iopub.execute_input":"2022-04-05T17:30:45.83423Z","iopub.status.idle":"2022-04-05T17:30:45.839816Z","shell.execute_reply.started":"2022-04-05T17:30:45.834185Z","shell.execute_reply":"2022-04-05T17:30:45.838837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"def augmented_data_creator(data):\n    num_batches = 500\n    # pass generator, outputtypes and num_batches\n    # args needs to be tuple of tensors\n    augmented_data = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32), args=(tf.constant(num_batches),))\n    # Now do the remaining tensorflow pipeline\n    augmented_data = augmented_data.map(lambda x, y: (x, tf.one_hot(tf.cast(y, tf.uint8), 3)))\n    augmented_data = augmented_data.map(lambda x,y: ((2*x-1), y))\n\n    return augmented_data\n\n\n# data pipeline to pre-process the images\ndef preprocessing_data(data):\n    'preprocesses the dataset'\n    #convert data from uint8 to float32\n    data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n    data = data.map(lambda img, target: (tf.image.resize(img, [128,128]), target))\n    data = data.map(lambda img, target: ((img/128.)-1., target))\n    data = data.map(lambda img, target: (img, tf.one_hot(target, depth=3)))\n    data = data.batch(64)\n\n    return data ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:50.671606Z","iopub.execute_input":"2022-04-10T12:21:50.672063Z","iopub.status.idle":"2022-04-10T12:21:50.683902Z","shell.execute_reply.started":"2022-04-10T12:21:50.672016Z","shell.execute_reply":"2022-04-10T12:21:50.682875Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#concatenate the original data and the augmented data, or only use the unaugmented data to compare training progress. Then cache, shuffle, prefetch\n#train_data = train_ds.apply(preprocessing_data)\n#unaugmented_train_data = train_data.cache().shuffle(64).prefetch(20)\n#train_data = train_data.concatenate(augmented_data).cache().shuffle(64).prefetch(20)\n\n# We only used the augmented data in the End\naugmented_data = augmented_data_creator(train_ds)\ntrain_data = augmented_data.cache().shuffle(64).prefetch(20)\ntest_data = test_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:50.685352Z","iopub.execute_input":"2022-04-10T12:21:50.685905Z","iopub.status.idle":"2022-04-10T12:21:50.903018Z","shell.execute_reply.started":"2022-04-10T12:21:50.685852Z","shell.execute_reply":"2022-04-10T12:21:50.902047Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained ResNetV2","metadata":{}},{"cell_type":"code","source":"# load a pretraines ResV2\npretrained_resv2 = tf.keras.applications.resnet_v2.ResNet101V2(include_top = False)\n\n## Freezing all earlier layers that represent low-level features\nfor layer in pretrained_resv2.layers[:128]:\n    layer.trainable = False\n\n# turn the model into a sequential to add layers for our need\ntuning_model = Sequential()\ntuning_model.add(pretrained_resv2)\ntuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\ntuning_model.add(Dense(256, 'relu'))\ntuning_model.add(Dense(3, 'softmax'))\n\ntuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T17:30:46.021492Z","iopub.execute_input":"2022-04-05T17:30:46.021739Z","iopub.status.idle":"2022-04-05T17:30:50.418508Z","shell.execute_reply.started":"2022-04-05T17:30:46.021704Z","shell.execute_reply":"2022-04-05T17:30:50.417791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained V3","metadata":{}},{"cell_type":"code","source":"# using the same pre-trained weights as suggested by another tutorial with the art movement data set\nv3_weights = '../input/keras-pretrained-models/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_v3 = tf.keras.applications.InceptionV3(input_shape = (128,128,3), include_top = False, weights = v3_weights)\n\n## Freezing all earlier layers that represent low-level features\nfor layer in pretrained_v3.layers[:64]:\n    layer.trainable = False\n\n# turn the model into a sequential to add layers for our need\ntuning_model = Sequential()\ntuning_model.add(pretrained_v3)\ntuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\ntuning_model.add(Dense(256, 'relu'))\ntuning_model.add(Dense(3, 'softmax'))\n\ntuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:34:13.869896Z","iopub.execute_input":"2022-04-04T15:34:13.870154Z","iopub.status.idle":"2022-04-04T15:34:14.638808Z","shell.execute_reply.started":"2022-04-04T15:34:13.870127Z","shell.execute_reply":"2022-04-04T15:34:14.637934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained VGG19","metadata":{}},{"cell_type":"code","source":"#trying a VGG19\npretrained_vgg19 = tf.keras.applications.vgg19.VGG19(input_shape = (128,128,3), include_top = False)\n\n## Freezing all earlier layers that represent low-level features\nfor layer in pretrained_vgg19.layers[:18]:\n    layer.trainable = False\n    \n# turn the model into a sequential to add layers for our need\ntuning_model = Sequential()\ntuning_model.add(pretrained_vgg19)\ntuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\ntuning_model.add(Dense(256, 'relu'))\ntuning_model.add(Dense(3, 'softmax'))\n\ntuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T17:30:54.882196Z","iopub.execute_input":"2022-04-04T17:30:54.882461Z","iopub.status.idle":"2022-04-04T17:30:54.971425Z","shell.execute_reply.started":"2022-04-04T17:30:54.882432Z","shell.execute_reply":"2022-04-04T17:30:54.970704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained ResNet50 - Simpler Architecture","metadata":{}},{"cell_type":"code","source":"# trying a pre-trained ResNet50\npretrained_res = tf.keras.applications.resnet50.ResNet50(include_top=False)\n\n## Freezing all earlier layers that represent low-level features\nfor layer in pretrained_res.layers[:32]:\n    layer.trainable = False\n\n# turn the model into a sequential to add layers for our need\ntuning_model = Sequential()\ntuning_model.add(pretrained_res)\ntuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\ntuning_model.add(Dense(256, 'relu'))\ntuning_model.add(Dense(3, 'softmax'))\n\ntuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:40:34.237357Z","iopub.execute_input":"2022-04-03T19:40:34.238152Z","iopub.status.idle":"2022-04-03T19:40:34.265529Z","shell.execute_reply.started":"2022-04-03T19:40:34.23811Z","shell.execute_reply":"2022-04-03T19:40:34.264478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuning_model.summary()\n# load previously saved weights to continue training. \n#tuning_model.load_weights(f\"saved_model_artstyle_discrimination_network{hyperparameter_string_res}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T17:30:50.419622Z","iopub.execute_input":"2022-04-05T17:30:50.421265Z","iopub.status.idle":"2022-04-05T17:30:50.448366Z","shell.execute_reply.started":"2022-04-05T17:30:50.421228Z","shell.execute_reply":"2022-04-05T17:30:50.447305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(model, input, target, loss_function, optimizer):\n    # loss_object and optimizer_object are instances of respective tensorflow classes\n    with tf.GradientTape() as tape:\n        prediction = model(input)#, train = True )\n        loss = loss_function(target, prediction)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n\ndef test(model, test_data, loss_function):\n    # test over complete test data\n\n    test_accuracy_aggregator = []\n    test_loss_aggregator = []\n\n    for (input, target) in test_data:\n        prediction = model(input)#, train = False)\n        sample_test_loss = loss_function(target, prediction)\n        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n        sample_test_accuracy = np.mean(sample_test_accuracy)\n        test_loss_aggregator.append(sample_test_loss.numpy())\n        test_accuracy_aggregator.append(sample_test_accuracy)\n\n    test_loss = tf.reduce_mean(test_loss_aggregator)\n    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n\n    return test_loss, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:21:50.904548Z","iopub.execute_input":"2022-04-10T12:21:50.904856Z","iopub.status.idle":"2022-04-10T12:21:50.915236Z","shell.execute_reply.started":"2022-04-10T12:21:50.904816Z","shell.execute_reply":"2022-04-10T12:21:50.913877Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# load tensorboard extension\n%load_ext tensorboard\n\n# clear all previous logs:\n!rm -rf ./logs/\n\n# define file-path for log file\nfile_path = \"final_project/discrimination_network\"\n\n# Define where to save the log\nhyperparameter_string = \"Adam_LR000025_resv2_Layersfrozen_32_cache\"\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ntrain_log_path = f\"logs/{file_path}/{hyperparameter_string}/{current_time}/train\"\ntest_log_path = f\"logs/{file_path}/{hyperparameter_string}/{current_time}/test\"\n\n# log writer for training metrics\ntrain_summary_writer = tf.summary.create_file_writer(train_log_path)\n\n# log writer for validation metrics\ntest_summary_writer = tf.summary.create_file_writer(test_log_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T17:31:03.001577Z","iopub.execute_input":"2022-04-05T17:31:03.002095Z","iopub.status.idle":"2022-04-05T17:31:03.878312Z","shell.execute_reply.started":"2022-04-05T17:31:03.002056Z","shell.execute_reply":"2022-04-05T17:31:03.877242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline model for comparison\ntf.keras.backend.clear_session()\n#%tensorboard --logdir logs/\n\n#assign train and test dataset\ntrain_dataset = train_data\ntest_dataset = test_data\n\n# for training with the unaugmented images in the training set included\n#train_dataset = unaugmented_train_data\n\n### Hyperparameter ################################################################################\nnum_epochs = 40\nlearning_rate = 25e-6\n\n# Assign the model.\nmodel = tuning_model\n\n# Initialize the loss, categorical cross entropy\ncross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n# Initialize the optimizer Adam, only adjusting the learning-rate\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n\n# Initialize lists for later visualization.\ntrain_losses = []\ntrain_accuracies = []\n\ntest_losses = []\ntest_accuracies = []\n\n\n#testing on the validation dataset once before we begin\ntest_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\ntest_losses.append(test_loss)\ntest_accuracies.append(test_accuracy)\n\n#check how model performs on train data once before we begin\ntrain_loss, train_accuracy = test(model, train_dataset, cross_entropy_loss)\ntrain_losses.append(train_loss)\ntrain_accuracies.append(train_accuracy)\n\nprint(f'Untrained Accuracy on Train Data {train_accuracies[-1]}')\n\n\n# We train for num_epochs epochs or until a certain accuracy for unseen data is met\nfor epoch in range(num_epochs):\n    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n\n    #training (and checking in with training)\n    epoch_loss_agg = []\n    for input,target in train_dataset:\n\n        #randomly crop images while training to enhance the amount of data\n        images = []\n        for img in input:\n            cropsize = 256\n            if np.min(img.shape[:-1]) < cropsize:\n                cropsize = np.min(img.shape[:-1])\n            img = tf.image.resize_with_crop_or_pad(img, cropsize, cropsize)\n            img = tf.image.resize(img, [128,128])\n            images.append(img)        \n        \n        images = tf.convert_to_tensor(images)\n\n        train_loss = train_step(model, images, target, cross_entropy_loss, optimizer)\n        epoch_loss_agg.append(train_loss)\n\n        \n    # track training loss\n    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n    \n    train_loss , train_accuracy = test(model, train_dataset, cross_entropy_loss)\n    train_accuracies.append(train_accuracy)\n    mean_train_loss = np.mean(epoch_loss_agg)\n    \n    print(f'Epoch: {str(epoch)} ending with accuracy on Training Set {train_accuracies[-1]}')\n\n    with train_summary_writer.as_default():\n        tf.summary.scalar(f\"Train Accuracy\", train_accuracy, step=epoch)\n\n    # testing, so we can track accuracy and test loss\n    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_accuracy)\n    \n    # to prevent excessive training, we stop the model, once the desired test-accuracy is reached\n    if test_accuracy > 0.85:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:49:54.311961Z","iopub.execute_input":"2022-04-05T21:49:54.312226Z","iopub.status.idle":"2022-04-05T21:49:54.742566Z","shell.execute_reply.started":"2022-04-05T21:49:54.3122Z","shell.execute_reply":"2022-04-05T21:49:54.74149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the Model and the weights","metadata":{}},{"cell_type":"code","source":"print(hyperparameter_string)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:57:41.137664Z","iopub.execute_input":"2022-04-10T12:57:41.138013Z","iopub.status.idle":"2022-04-10T12:57:41.165838Z","shell.execute_reply.started":"2022-04-10T12:57:41.137979Z","shell.execute_reply":"2022-04-10T12:57:41.164850Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"hyperparameter_string = \"Adam_LR000025_resv2_Layersfrozen_128_cache_ACC85\"\ntuning_model.save_weights(f\"saved_model_artstyle_discrimination_network{hyperparameter_string}\", save_format=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T18:49:07.133527Z","iopub.execute_input":"2022-04-05T18:49:07.133806Z","iopub.status.idle":"2022-04-05T18:49:08.112511Z","shell.execute_reply.started":"2022-04-05T18:49:07.133773Z","shell.execute_reply":"2022-04-05T18:49:08.111669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tuning_model.trainable = False\n#tuning_model.save(\"art_transfer_discrimination_model_resV2_Adam_LR000025_frozenlayers128_ACC85.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T18:49:42.813951Z","iopub.execute_input":"2022-04-05T18:49:42.814623Z","iopub.status.idle":"2022-04-05T18:49:43.709111Z","shell.execute_reply.started":"2022-04-05T18:49:42.814588Z","shell.execute_reply":"2022-04-05T18:49:43.70821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the trained Model to see how our Art-style-Transfer Performs objective","metadata":{}},{"cell_type":"code","source":"art_transfer_discrimination_model = tf.keras.models.load_model(\"../input/art-transfer-discrimination-model-resv2/art_transfer_discrimination_model_resV2_Adam_LR000025_frozenlayers128_ACC85.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:22:28.143489Z","iopub.execute_input":"2022-04-10T12:22:28.143762Z","iopub.status.idle":"2022-04-10T12:22:34.361904Z","shell.execute_reply.started":"2022-04-10T12:22:28.143732Z","shell.execute_reply":"2022-04-10T12:22:34.360977Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"art_transfer_discrimination_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T10:13:11.600442Z","iopub.execute_input":"2022-04-10T10:13:11.600709Z","iopub.status.idle":"2022-04-10T10:13:11.629386Z","shell.execute_reply.started":"2022-04-10T10:13:11.600681Z","shell.execute_reply":"2022-04-10T10:13:11.628466Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"markdown","source":"### Loading the pictures","metadata":{}},{"cell_type":"code","source":"# dictionary for possible later translations\nstyle_to_number = {\n    'cubism': 0,\n    'expressionism' : 1,\n    'romanticism' : 2\n}\n\n\ndef load_images(path):\n    '''Load images and create corresponding numerical labels for the classes\n    Input: \n            path: a path\n            \n    Output:\n            images: a list of arrays \n            labels: a list of numerical labels'''\n    \n    \n    # load all path for the images, respectively to their epoch\n    cubism_paths = glob.glob(path + '/cubism/*')\n    expressionism_paths = glob.glob(path + '/expressionism/*')\n    romanticism_paths = glob.glob(path + '/romanticism/*')\n\n    combined_paths = [cubism_paths, expressionism_paths, romanticism_paths]\n    images = []\n    labels = []\n    \n    # load images and create art-style-corresponding label list for them\n    for i, art_style in enumerate(combined_paths):\n        for image_path in art_style:\n            image = np.asarray(tf.keras.preprocessing.image.load_img(image_path))     \n            images.append(image)\n            labels.append(i)\n\n    return images, labels\n\n\n# Generators to merge the labels and images from the datasets\ndef vgg_data_gen():\n    for i, image in enumerate(vgg_images):\n        yield image, vgg_labels[i]\n\n        \ndef res_data_gen():\n    for i, image in enumerate(res_images):\n        yield image, res_labels[i]\n\n        \ndef vgg_fn_data_gen():\n    for i, image in enumerate(vgg_fn_images):\n        yield image, vgg_fn_labels[i]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:35:57.510824Z","iopub.execute_input":"2022-04-10T13:35:57.511111Z","iopub.status.idle":"2022-04-10T13:35:57.522254Z","shell.execute_reply.started":"2022-04-10T13:35:57.511080Z","shell.execute_reply":"2022-04-10T13:35:57.521284Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### Determine Performence","metadata":{}},{"cell_type":"code","source":"# define the entropy\ncross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n\n# load the dataset from the VGG with the content image as input\nvgg_images, vgg_labels = load_images(path = '../input/vgg19-augmented-from-content-image/VGG19_augmented_from_content_image')\n\n# create the dataset from images and labels\nvgg_ds = tf.data.Dataset.from_generator(vgg_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n                                                             tf.TensorSpec(shape=(), dtype=tf.int32)))\n\n# preprocessing\nvgg_data = vgg_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:34:33.487380Z","iopub.execute_input":"2022-04-10T13:34:33.487973Z","iopub.status.idle":"2022-04-10T13:34:38.783498Z","shell.execute_reply.started":"2022-04-10T13:34:33.487923Z","shell.execute_reply":"2022-04-10T13:34:38.782654Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# load the dataset from the VGG with the noise image as input\nvgg_fn_images, vgg_fn_labels = load_images(path = '../input/vgg19-augmented-from-noise/VGG19_augmented_from_noise')\n\n# create the dataset from images and labels\nvgg_fn_ds = tf.data.Dataset.from_generator(vgg_fn_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n                                                             tf.TensorSpec(shape=(), dtype=tf.int32)))\n\n# preprocessing\nvgg_fn_data = vgg_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:34:38.785387Z","iopub.execute_input":"2022-04-10T13:34:38.785616Z","iopub.status.idle":"2022-04-10T13:34:46.142187Z","shell.execute_reply.started":"2022-04-10T13:34:38.785589Z","shell.execute_reply":"2022-04-10T13:34:46.141334Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# load the dataset from the ResNet\nres_images, res_labels = load_images(path = '../input/resnet-style-transferred-images/Results Pretrained ResNet50')\n\n# create the dataset from images and labels\nres_ds = tf.data.Dataset.from_generator(res_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n                                                             tf.TensorSpec(shape=(), dtype=tf.int32)))\n\n# preprocessing\nres_data = res_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:49:09.412317Z","iopub.execute_input":"2022-04-10T12:49:09.412611Z","iopub.status.idle":"2022-04-10T12:49:09.532002Z","shell.execute_reply.started":"2022-04-10T12:49:09.412581Z","shell.execute_reply":"2022-04-10T12:49:09.530948Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"#### Peak at the data","metadata":{}},{"cell_type":"code","source":"#peek at the images and labels\nprint(plt.imshow(vgg_fn_images[1]), vgg_fn_labels[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:34:50.209495Z","iopub.execute_input":"2022-04-10T13:34:50.209816Z","iopub.status.idle":"2022-04-10T13:34:50.486465Z","shell.execute_reply.started":"2022-04-10T13:34:50.209785Z","shell.execute_reply":"2022-04-10T13:34:50.485559Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#peek at the images and labels\nprint(plt.imshow(vgg_images[1]), vgg_labels[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:34:55.333450Z","iopub.execute_input":"2022-04-10T13:34:55.333728Z","iopub.status.idle":"2022-04-10T13:34:55.613365Z","shell.execute_reply.started":"2022-04-10T13:34:55.333693Z","shell.execute_reply":"2022-04-10T13:34:55.612451Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#peek at the images and labels\nprint(plt.imshow(res_images[1]), res_labels[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T10:14:16.298831Z","iopub.execute_input":"2022-04-10T10:14:16.299153Z","iopub.status.idle":"2022-04-10T10:14:16.523217Z","shell.execute_reply.started":"2022-04-10T10:14:16.299121Z","shell.execute_reply":"2022-04-10T10:14:16.522129Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"markdown","source":"### Calculate the Accuracy of the Network with different Style Transferred Images","metadata":{}},{"cell_type":"code","source":"#calculate the performence of the network on differnt style-transferred images:\n#VGG19, content image as input\nvgg_loss, vgg_accuracy = test(art_transfer_discrimination_model, vgg_data, cross_entropy_loss)\nprint(f'VGG19 Accuracy with Content Image as Input: {vgg_accuracy}.')\n\n#VGG19, noise image as input\nvgg_fn_loss, vgg_fn_accuracy = test(art_transfer_discrimination_model, vgg_fn_data, cross_entropy_loss)\nprint(f'VGG19 Accuracy with Noise Image as Input: {vgg_fn_accuracy}.')\n\n#ResNet50, content image as input\nres_loss, res_accuracy = test(art_transfer_discrimination_model, res_data, cross_entropy_loss)\nprint(f'Res Accuracy with Content Image as Input: {res_accuracy}.')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:34:58.610798Z","iopub.execute_input":"2022-04-10T13:34:58.611073Z","iopub.status.idle":"2022-04-10T13:35:20.381805Z","shell.execute_reply.started":"2022-04-10T13:34:58.611044Z","shell.execute_reply":"2022-04-10T13:35:20.380781Z"},"trusted":true},"execution_count":61,"outputs":[]}]}