{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "ASEWA_Art_Epoch_Discrimination_ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zEFrbL0TdBQq"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASEWA - Art Epoch Discrimination ResNet\n",
        "Calculates the accuracies of the art style classifications by the Art-Transfer-Discrimination model\n",
        "---\n",
        "Janina Klarmann, Laura KÃ¼hl"
      ],
      "metadata": {
        "id": "Y8OWkQoUdIDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import math\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# for looking at files\n",
        "import glob\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:14:18.628398Z",
          "iopub.execute_input": "2022-04-11T02:14:18.628671Z",
          "iopub.status.idle": "2022-04-11T02:14:24.920185Z",
          "shell.execute_reply.started": "2022-04-11T02:14:18.628644Z",
          "shell.execute_reply": "2022-04-11T02:14:24.919417Z"
        },
        "trusted": true,
        "id": "0wXV7kSfdBPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "Y2XxHvS8dBPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_to_number = {\n",
        "    'cubism': 0,\n",
        "    'expressionism' : 1,\n",
        "    'romanticism' : 2\n",
        "}\n",
        "\n",
        "def load_images(folder):\n",
        "    '''Load images and create corresponding numerical labels for the classes\n",
        "    Input: path: a path.\n",
        "            \n",
        "    Output: images: a list of arrays. labels: a list of numerical labels'''\n",
        "    \n",
        "    # load all path for the images, respectively to their epoch\n",
        "    path = '../input/art-movements/dataset/' + folder\n",
        "    \n",
        "    cubism_paths = glob.glob(path + '/cubism/*')\n",
        "    expressionism_paths = glob.glob(path + '/expressionism/*')\n",
        "    romanticism_paths = glob.glob(path + '/romanticism/*')\n",
        "    \n",
        "    combined_paths = [cubism_paths, expressionism_paths, romanticism_paths]\n",
        "    \n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # load images and create art-style-corresponding label list for them\n",
        "    for i, art_style in enumerate(combined_paths):\n",
        "        for image_path in art_style:\n",
        "            image = np.asarray(tf.keras.preprocessing.image.load_img(image_path))     \n",
        "            images.append(image)\n",
        "            labels.append(i)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def resize_images(images):\n",
        "    '''Images get resized into a uniform size'''\n",
        "    \n",
        "    return [tf.image.resize(image, [128,128]) for image in images]\n",
        "\n",
        "\n",
        "def crop_images(images):\n",
        "    '''Crop the image in the biggest possible square\n",
        "        \n",
        "            Input: Array of images\n",
        "            \n",
        "            Output: Array if square image'''\n",
        "    \n",
        "    cropped = []\n",
        "    \n",
        "    for image in images:\n",
        "        shape = np.min(image.shape[:-1])\n",
        "        cropped_image = tf.image.resize_with_crop_or_pad(image, shape, shape)\n",
        "        cropped.append(cropped_image)\n",
        "\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# we did not use it, but it could be used to create more examples for training the network. \n",
        "# It is questionable if the style may gets distorted too much\n",
        "def random_crop(images):\n",
        "    '''Randomly crop all images into a uiform size'''\n",
        "    return [tf.image.random_crop(image, size=[128, 128, 3]) for image in images]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:14:24.922155Z",
          "iopub.execute_input": "2022-04-11T02:14:24.922405Z",
          "iopub.status.idle": "2022-04-11T02:14:24.934699Z",
          "shell.execute_reply.started": "2022-04-11T02:14:24.922371Z",
          "shell.execute_reply": "2022-04-11T02:14:24.933477Z"
        },
        "trusted": true,
        "id": "R3oQQDvidBP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create arrays of labels ad images\n",
        "train_images, train_labels = load_images(folder='train')\n",
        "test_images, test_labels = load_images(folder = 'test')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:14:24.935863Z",
          "iopub.execute_input": "2022-04-11T02:14:24.936249Z",
          "iopub.status.idle": "2022-04-11T02:14:57.728952Z",
          "shell.execute_reply.started": "2022-04-11T02:14:24.936215Z",
          "shell.execute_reply": "2022-04-11T02:14:57.728229Z"
        },
        "trusted": true,
        "id": "xgQNDx6MdBP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crop and resize images into a uniform shape to be able to create a tensorflow dataset\n",
        "train_images_cropped = crop_images(train_images)\n",
        "train_images_resized = resize_images(train_images_cropped)\n",
        "test_images_cropped = crop_images(test_images)\n",
        "test_images_resized = resize_images(test_images_cropped)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:15:39.812234Z",
          "iopub.execute_input": "2022-04-11T02:15:39.812490Z",
          "iopub.status.idle": "2022-04-11T02:15:48.306267Z",
          "shell.execute_reply.started": "2022-04-11T02:15:39.812461Z",
          "shell.execute_reply": "2022-04-11T02:15:48.305512Z"
        },
        "trusted": true,
        "id": "CxsC4M1QdBQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Datasets "
      ],
      "metadata": {
        "id": "5NPbNNODdBQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator do merge the images and labels for train and test-datasets\n",
        "def train_data_gen():\n",
        "    for i, image in enumerate(train_images):\n",
        "        yield image, train_labels[i]\n",
        "\n",
        "def test_data_gen():\n",
        "    for i, image in enumerate(test_images):\n",
        "        yield image, test_labels[i]\n",
        "\n",
        "\n",
        "# creste a tf datasets from the loaded images and the labels\n",
        "train_ds = tf.data.Dataset.from_generator(train_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n",
        "                                                             tf.TensorSpec(shape=(), dtype=tf.int32))\n",
        "                                                             )\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(test_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n",
        "                                                             tf.TensorSpec(shape=(), dtype=tf.int32))\n",
        "                                                             )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:15:57.146415Z",
          "iopub.execute_input": "2022-04-11T02:15:57.146678Z",
          "iopub.status.idle": "2022-04-11T02:15:57.213045Z",
          "shell.execute_reply.started": "2022-04-11T02:15:57.146650Z",
          "shell.execute_reply": "2022-04-11T02:15:57.212363Z"
        },
        "trusted": true,
        "id": "CjzSNl5TdBQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plottting some Images"
      ],
      "metadata": {
        "id": "Xpb9JCdqdBQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(2*(train_images[1]/256)-1) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:02.562337Z",
          "iopub.execute_input": "2022-04-11T02:16:02.562851Z",
          "iopub.status.idle": "2022-04-11T02:16:03.023413Z",
          "shell.execute_reply.started": "2022-04-11T02:16:02.562802Z",
          "shell.execute_reply": "2022-04-11T02:16:03.021577Z"
        },
        "trusted": true,
        "id": "voDx_sUBdBQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_images_cropped[1]) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:04.993120Z",
          "iopub.execute_input": "2022-04-11T02:16:04.993386Z",
          "iopub.status.idle": "2022-04-11T02:16:05.276936Z",
          "shell.execute_reply.started": "2022-04-11T02:16:04.993356Z",
          "shell.execute_reply": "2022-04-11T02:16:05.273174Z"
        },
        "trusted": true,
        "id": "i7uqo6KidBQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(2*(train_images_resized[1]/256)-1) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:06.552074Z",
          "iopub.execute_input": "2022-04-11T02:16:06.552814Z",
          "iopub.status.idle": "2022-04-11T02:16:06.788484Z",
          "shell.execute_reply.started": "2022-04-11T02:16:06.552776Z",
          "shell.execute_reply": "2022-04-11T02:16:06.787731Z"
        },
        "trusted": true,
        "id": "Wz6iBA3idBQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "voNIjl80dBQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the generator we \n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    #rotation_range=40,\n",
        "    brightness_range=(0.5,1.5),\n",
        "    #width_shift_range=0.15,\n",
        "    #height_shift_range=0.15,\n",
        "    shear_range=5,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = True,\n",
        "    zoom_range = [0.7, 1]\n",
        ")\n",
        "\n",
        "# create an iterator using datagen.flow\n",
        "train_images_resized = np.asarray(train_images_resized)\n",
        "train_labels = np.asarray(train_labels)\n",
        "train_generator = datagen.flow(train_images_resized, train_labels, batch_size=64)\n",
        "\n",
        "\n",
        "def generator(num_batches):\n",
        "    for i, train_tuple in enumerate(train_generator):\n",
        "        yield train_tuple\n",
        "        if i >= num_batches:\n",
        "            return"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:10.358393Z",
          "iopub.execute_input": "2022-04-11T02:16:10.358651Z",
          "iopub.status.idle": "2022-04-11T02:16:10.537514Z",
          "shell.execute_reply.started": "2022-04-11T02:16:10.358623Z",
          "shell.execute_reply": "2022-04-11T02:16:10.536776Z"
        },
        "trusted": true,
        "id": "obVSxuWhdBQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plott Augmented Data"
      ],
      "metadata": {
        "id": "9Ij7mtwUdBQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Plotting Augmented Images\n",
        "\n",
        "# plot images of first batch\n",
        "fig, ax = plt.subplots(4,8,figsize=(20,10))\n",
        "fig.tight_layout()\n",
        "ax = ax.flatten()\n",
        "for img_b, label_b in train_generator:\n",
        "    for i in range(32):   \n",
        "        img = img_b[i]\n",
        "        l = label_b[i]\n",
        "        \n",
        "        ax[i].imshow(img)  \n",
        "        ax[i].set_title((img.shape))\n",
        "        ax[i].axis(\"off\")\n",
        "\n",
        "    break   \n",
        "\n",
        "fig, ax = plt.subplots(4,8,figsize=(20,10))\n",
        "fig.tight_layout()\n",
        "ax = ax.flatten()\n",
        "for img_b, label_b in train_generator:\n",
        "    for i in range(32):   \n",
        "        img = img_b[i]\n",
        "        l = label_b[i]\n",
        "        ax[i].imshow(img)  \n",
        "        ax[i].set_title((img.shape))\n",
        "        ax[i].axis(\"off\")\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:20.009783Z",
          "iopub.execute_input": "2022-04-11T02:16:20.010054Z",
          "iopub.status.idle": "2022-04-11T02:16:24.883604Z",
          "shell.execute_reply.started": "2022-04-11T02:16:20.010024Z",
          "shell.execute_reply": "2022-04-11T02:16:24.870690Z"
        },
        "trusted": true,
        "id": "uZJoLMFsdBQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess data"
      ],
      "metadata": {
        "id": "JGgnse4rdBQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augmented_data_creator(data):\n",
        "    num_batches = 500\n",
        "    # pass generator, outputtypes and num_batches\n",
        "    # args needs to be tuple of tensors\n",
        "    augmented_data = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32), args=(tf.constant(num_batches),))\n",
        "    # Now do the remaining tensorflow pipeline\n",
        "    augmented_data = augmented_data.map(lambda x, y: (x, tf.one_hot(tf.cast(y, tf.uint8), 3)))\n",
        "    augmented_data = augmented_data.map(lambda x,y: ((2*x-1), y))\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "# data pipeline to pre-process the images\n",
        "def preprocessing_data(data):\n",
        "    'preprocesses the dataset'\n",
        "    #convert data from uint8 to float32\n",
        "    data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "    data = data.map(lambda img, target: (tf.image.resize(img, [128,128]), target))\n",
        "    data = data.map(lambda img, target: ((img/128.)-1., target))\n",
        "    data = data.map(lambda img, target: (img, tf.one_hot(target, depth=3)))\n",
        "    data = data.batch(64)\n",
        "\n",
        "    return data "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:41.388469Z",
          "iopub.execute_input": "2022-04-11T02:16:41.388731Z",
          "iopub.status.idle": "2022-04-11T02:16:41.400059Z",
          "shell.execute_reply.started": "2022-04-11T02:16:41.388702Z",
          "shell.execute_reply": "2022-04-11T02:16:41.397807Z"
        },
        "trusted": true,
        "id": "CjB4aIvRdBQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate the original data and the augmented data, or only use the unaugmented data to compare training progress. Then cache, shuffle, prefetch\n",
        "#train_data = train_ds.apply(preprocessing_data)\n",
        "#unaugmented_train_data = train_data.cache().shuffle(64).prefetch(20)\n",
        "#train_data = train_data.concatenate(augmented_data).cache().shuffle(64).prefetch(20)\n",
        "\n",
        "# We only used the augmented data in the End\n",
        "augmented_data = augmented_data_creator(train_ds)\n",
        "train_data = augmented_data.cache().shuffle(64).prefetch(20)\n",
        "test_data = test_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:16:48.641025Z",
          "iopub.execute_input": "2022-04-11T02:16:48.641279Z",
          "iopub.status.idle": "2022-04-11T02:16:48.803106Z",
          "shell.execute_reply.started": "2022-04-11T02:16:48.641251Z",
          "shell.execute_reply": "2022-04-11T02:16:48.802428Z"
        },
        "trusted": true,
        "id": "O6mNeboedBQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained ResNetV2"
      ],
      "metadata": {
        "id": "0AZpe8itdBQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a pretraines ResV2\n",
        "pretrained_resv2 = tf.keras.applications.resnet_v2.ResNet101V2(include_top = False)\n",
        "\n",
        "## Freezing all earlier layers that represent low-level features\n",
        "for layer in pretrained_resv2.layers[:128]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# turn the model into a sequential to add layers for our need\n",
        "tuning_model = Sequential()\n",
        "tuning_model.add(pretrained_resv2)\n",
        "tuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "tuning_model.add(Dense(256, 'relu'))\n",
        "tuning_model.add(Dense(3, 'softmax'))\n",
        "\n",
        "tuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:17:32.697620Z",
          "iopub.execute_input": "2022-04-11T02:17:32.697873Z",
          "iopub.status.idle": "2022-04-11T02:17:36.906003Z",
          "shell.execute_reply.started": "2022-04-11T02:17:32.697843Z",
          "shell.execute_reply": "2022-04-11T02:17:36.905261Z"
        },
        "trusted": true,
        "id": "I3rv4AoadBQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained V3"
      ],
      "metadata": {
        "id": "hweThYlkdBQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the same pre-trained weights as suggested by another tutorial with the art movement data set\n",
        "v3_weights = '../input/keras-pretrained-models/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pretrained_v3 = tf.keras.applications.InceptionV3(input_shape = (128,128,3), include_top = False, weights = v3_weights)\n",
        "\n",
        "## Freezing all earlier layers that represent low-level features\n",
        "for layer in pretrained_v3.layers[:64]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# turn the model into a sequential to add layers for our need\n",
        "tuning_model = Sequential()\n",
        "tuning_model.add(pretrained_v3)\n",
        "tuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "tuning_model.add(Dense(256, 'relu'))\n",
        "tuning_model.add(Dense(3, 'softmax'))\n",
        "\n",
        "tuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.225164Z",
          "iopub.status.idle": "2022-04-11T00:58:22.225969Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.225736Z",
          "shell.execute_reply": "2022-04-11T00:58:22.225762Z"
        },
        "trusted": true,
        "id": "k9aHv0KRdBQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained VGG19"
      ],
      "metadata": {
        "id": "84iDQ-GDdBQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trying a VGG19\n",
        "pretrained_vgg19 = tf.keras.applications.vgg19.VGG19(input_shape = (128,128,3), include_top = False)\n",
        "\n",
        "## Freezing all earlier layers that represent low-level features\n",
        "for layer in pretrained_vgg19.layers[:18]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# turn the model into a sequential to add layers for our need\n",
        "tuning_model = Sequential()\n",
        "tuning_model.add(pretrained_vgg19)\n",
        "tuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "tuning_model.add(Dense(256, 'relu'))\n",
        "tuning_model.add(Dense(3, 'softmax'))\n",
        "\n",
        "tuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.227027Z",
          "iopub.status.idle": "2022-04-11T00:58:22.22795Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.227732Z",
          "shell.execute_reply": "2022-04-11T00:58:22.227762Z"
        },
        "trusted": true,
        "id": "s8ytnLTYdBQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained ResNet50 - Simpler Architecture"
      ],
      "metadata": {
        "id": "_jDZvUx8dBQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trying a pre-trained ResNet50\n",
        "pretrained_res = tf.keras.applications.resnet50.ResNet50(include_top=False)\n",
        "\n",
        "## Freezing all earlier layers that represent low-level features\n",
        "for layer in pretrained_res.layers[:32]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# turn the model into a sequential to add layers for our need\n",
        "tuning_model = Sequential()\n",
        "tuning_model.add(pretrained_res)\n",
        "tuning_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "tuning_model.add(Dense(256, 'relu'))\n",
        "tuning_model.add(Dense(3, 'softmax'))\n",
        "\n",
        "tuning_model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics=['acc'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.229415Z",
          "iopub.status.idle": "2022-04-11T00:58:22.230069Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.229839Z",
          "shell.execute_reply": "2022-04-11T00:58:22.229864Z"
        },
        "trusted": true,
        "id": "DcSEd9vDdBQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_model.summary()\n",
        "# load previously saved weights to continue training. \n",
        "#tuning_model.load_weights(f\"saved_model_artstyle_discrimination_network{hyperparameter_string_res}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.231279Z",
          "iopub.status.idle": "2022-04-11T00:58:22.231917Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.23168Z",
          "shell.execute_reply": "2022-04-11T00:58:22.231705Z"
        },
        "trusted": true,
        "id": "WlXnqtNcdBQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(input)#, train = True )\n",
        "        loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "    # test over complete test data\n",
        "\n",
        "    test_accuracy_aggregator = []\n",
        "    test_loss_aggregator = []\n",
        "\n",
        "    for (input, target) in test_data:\n",
        "        prediction = model(input)#, train = False)\n",
        "        sample_test_loss = loss_function(target, prediction)\n",
        "        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "        test_accuracy_aggregator.append(sample_test_accuracy)\n",
        "\n",
        "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:18:31.938868Z",
          "iopub.execute_input": "2022-04-11T02:18:31.939357Z",
          "iopub.status.idle": "2022-04-11T02:18:31.949574Z",
          "shell.execute_reply.started": "2022-04-11T02:18:31.939317Z",
          "shell.execute_reply": "2022-04-11T02:18:31.947865Z"
        },
        "trusted": true,
        "id": "cpMmGhPrdBQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#assign train and test dataset\n",
        "train_dataset = train_data\n",
        "test_dataset = test_data\n",
        "\n",
        "# for training with the unaugmented images in the training set included\n",
        "#train_dataset = unaugmented_train_data\n",
        "\n",
        "### Hyperparameter ################################################################################\n",
        "num_epochs = 40\n",
        "learning_rate = 25e-6\n",
        "\n",
        "# Assign the model.\n",
        "model = tuning_model\n",
        "\n",
        "# Initialize the loss, categorical cross entropy\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer Adam, only adjusting the learning-rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "\n",
        "#testing on the validation dataset once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, train_accuracy = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "train_accuracies.append(train_accuracy)\n",
        "\n",
        "print(f'Untrained Accuracy on Train Data {train_accuracies[-1]}')\n",
        "\n",
        "\n",
        "# We train for num_epochs epochs or until a certain accuracy for unseen data is met\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "\n",
        "        #randomly crop images while training to enhance the amount of data\n",
        "        images = []\n",
        "        for img in input:\n",
        "            cropsize = 256\n",
        "            if np.min(img.shape[:-1]) < cropsize:\n",
        "                cropsize = np.min(img.shape[:-1])\n",
        "            img = tf.image.resize_with_crop_or_pad(img, cropsize, cropsize)\n",
        "            img = tf.image.resize(img, [128,128])\n",
        "            images.append(img)        \n",
        "        \n",
        "        images = tf.convert_to_tensor(images)\n",
        "\n",
        "        train_loss = train_step(model, images, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "\n",
        "        \n",
        "    # track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "    \n",
        "    train_loss , train_accuracy = test(model, train_dataset, cross_entropy_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    mean_train_loss = np.mean(epoch_loss_agg)\n",
        "    \n",
        "    print(f'Epoch: {str(epoch)} ending with accuracy on Training Set {train_accuracies[-1]}')\n",
        "\n",
        "    # testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    \n",
        "    # to prevent excessive training, we stop the model, once the desired test-accuracy is reached\n",
        "    if test_accuracy > 0.85:\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T01:07:47.77617Z",
          "iopub.execute_input": "2022-04-11T01:07:47.776425Z",
          "iopub.status.idle": "2022-04-11T01:11:23.081765Z",
          "shell.execute_reply.started": "2022-04-11T01:07:47.776392Z",
          "shell.execute_reply": "2022-04-11T01:11:23.080542Z"
        },
        "trusted": true,
        "id": "7XPgu8SBdBQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "# Visualize accuracy and loss for training and test data. As \n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(train_accuracies)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy Baseline\")\n",
        "plt.legend((line1, line2, line3),(\"training losses\", \"training accuracies\", \"test accuracy\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T01:53:37.193375Z",
          "iopub.execute_input": "2022-04-11T01:53:37.193977Z",
          "iopub.status.idle": "2022-04-11T01:53:37.212715Z",
          "shell.execute_reply.started": "2022-04-11T01:53:37.193934Z",
          "shell.execute_reply": "2022-04-11T01:53:37.211332Z"
        },
        "trusted": true,
        "id": "jDmz24ohdBQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Model and the weights"
      ],
      "metadata": {
        "id": "jqbx-SkmdBQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(hyperparameter_string)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.238705Z",
          "iopub.status.idle": "2022-04-11T00:58:22.239336Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.239106Z",
          "shell.execute_reply": "2022-04-11T00:58:22.23913Z"
        },
        "trusted": true,
        "id": "Vj9vKlNYdBQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_string = \"Adam_LR000025_resv2_Layersfrozen_128_cache_ACC85\"\n",
        "tuning_model.save_weights(f\"saved_model_artstyle_discrimination_network{hyperparameter_string}\", save_format=\"tf\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.240748Z",
          "iopub.status.idle": "2022-04-11T00:58:22.241379Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.241145Z",
          "shell.execute_reply": "2022-04-11T00:58:22.241171Z"
        },
        "trusted": true,
        "id": "bXracrzVdBQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tuning_model.trainable = False\n",
        "#tuning_model.save(\"art_transfer_discrimination_model_resV2_Adam_LR000025_frozenlayers128_ACC85.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T00:58:22.242602Z",
          "iopub.status.idle": "2022-04-11T00:58:22.243227Z",
          "shell.execute_reply.started": "2022-04-11T00:58:22.242991Z",
          "shell.execute_reply": "2022-04-11T00:58:22.243016Z"
        },
        "trusted": true,
        "id": "lQuOIAnadBQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the trained Model to see how our Art-style-Transfer Performs objective"
      ],
      "metadata": {
        "id": "VAKY2FNfdBQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "art_transfer_discrimination_model = tf.keras.models.load_model(\"../input/art-transfer-discrimination-model-resv2/art_transfer_discrimination_model_resV2_Adam_LR000025_frozenlayers128_ACC85.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:18:57.269795Z",
          "iopub.execute_input": "2022-04-11T02:18:57.270063Z",
          "iopub.status.idle": "2022-04-11T02:19:02.430268Z",
          "shell.execute_reply.started": "2022-04-11T02:18:57.270032Z",
          "shell.execute_reply": "2022-04-11T02:19:02.429440Z"
        },
        "trusted": true,
        "id": "WH2_lDI0dBQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "art_transfer_discrimination_model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:19:06.064524Z",
          "iopub.execute_input": "2022-04-11T02:19:06.065115Z",
          "iopub.status.idle": "2022-04-11T02:19:06.094776Z",
          "shell.execute_reply.started": "2022-04-11T02:19:06.065074Z",
          "shell.execute_reply": "2022-04-11T02:19:06.093904Z"
        },
        "trusted": true,
        "id": "Vz7xeyindBQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the pictures"
      ],
      "metadata": {
        "id": "1T-yU0CzdBQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary for possible later translations\n",
        "style_to_number = {\n",
        "    'cubism': 0,\n",
        "    'expressionism' : 1,\n",
        "    'romanticism' : 2\n",
        "}\n",
        "\n",
        "\n",
        "def load_images(path):\n",
        "    '''Load images and create corresponding numerical labels for the classes\n",
        "    Input: \n",
        "            path: a path\n",
        "            \n",
        "    Output:\n",
        "            images: a list of arrays \n",
        "            labels: a list of numerical labels'''\n",
        "    \n",
        "    \n",
        "    # load all path for the images, respectively to their epoch\n",
        "    cubism_paths = glob.glob(path + '/cubism/*')\n",
        "    expressionism_paths = glob.glob(path + '/expressionism/*')\n",
        "    romanticism_paths = glob.glob(path + '/romanticism/*')\n",
        "\n",
        "    combined_paths = [cubism_paths, expressionism_paths, romanticism_paths]\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    # load images and create art-style-corresponding label list for them\n",
        "    for i, art_style in enumerate(combined_paths):\n",
        "        for image_path in art_style:\n",
        "            image = np.asarray(tf.keras.preprocessing.image.load_img(image_path))     \n",
        "            images.append(image)\n",
        "            labels.append(i)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "# Generators to merge the labels and images from the datasets\n",
        "def vgg_data_gen():\n",
        "    for i, image in enumerate(vgg_images):\n",
        "        yield image, vgg_labels[i]\n",
        "\n",
        "        \n",
        "def res_data_gen():\n",
        "    for i, image in enumerate(res_images):\n",
        "        yield image, res_labels[i]\n",
        "\n",
        "        \n",
        "def vgg_fn_data_gen():\n",
        "    for i, image in enumerate(vgg_fn_images):\n",
        "        yield image, vgg_fn_labels[i]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:19:10.381465Z",
          "iopub.execute_input": "2022-04-11T02:19:10.382076Z",
          "iopub.status.idle": "2022-04-11T02:19:10.390999Z",
          "shell.execute_reply.started": "2022-04-11T02:19:10.382037Z",
          "shell.execute_reply": "2022-04-11T02:19:10.390335Z"
        },
        "trusted": true,
        "id": "hQlhp34CdBQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine Performence"
      ],
      "metadata": {
        "id": "f08WMlgrdBQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the entropy\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# load the dataset from the VGG with the content image as input\n",
        "vgg_images, vgg_labels = load_images(path = '../input/vgg19-augmented-from-content-image/Results_VGG19_augmented_from_content_image')\n",
        "\n",
        "# create the dataset from images and labels\n",
        "vgg_ds = tf.data.Dataset.from_generator(vgg_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n",
        "                                                             tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "# preprocessing\n",
        "vgg_data = vgg_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:01.279962Z",
          "iopub.execute_input": "2022-04-11T02:21:01.280705Z",
          "iopub.status.idle": "2022-04-11T02:21:07.513793Z",
          "shell.execute_reply.started": "2022-04-11T02:21:01.280664Z",
          "shell.execute_reply": "2022-04-11T02:21:07.513094Z"
        },
        "trusted": true,
        "id": "nKMCzIHfdBQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset from the VGG with the noise image as input\n",
        "vgg_fn_images, vgg_fn_labels = load_images(path = '../input/vgg19-augmented-from-noise/Results_VGG19_augmented_from_noise')\n",
        "\n",
        "# create the dataset from images and labels\n",
        "vgg_fn_ds = tf.data.Dataset.from_generator(vgg_fn_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n",
        "                                                             tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "# preprocessing\n",
        "vgg_fn_data = vgg_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:23.058124Z",
          "iopub.execute_input": "2022-04-11T02:21:23.058385Z",
          "iopub.status.idle": "2022-04-11T02:21:29.916513Z",
          "shell.execute_reply.started": "2022-04-11T02:21:23.058357Z",
          "shell.execute_reply": "2022-04-11T02:21:29.915776Z"
        },
        "trusted": true,
        "id": "eHmniBk7dBQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset from the ResNet\n",
        "res_images, res_labels = load_images(path = '../input/resnet-style-transferred-images/Results Pretrained ResNet50')\n",
        "\n",
        "# create the dataset from images and labels\n",
        "res_ds = tf.data.Dataset.from_generator(res_data_gen, output_signature=(tf.TensorSpec(shape=(None, None, 3)),\n",
        "                                                             tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "# preprocessing\n",
        "res_data = res_ds.apply(preprocessing_data).cache().shuffle(64).prefetch(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:33.634160Z",
          "iopub.execute_input": "2022-04-11T02:21:33.634412Z",
          "iopub.status.idle": "2022-04-11T02:21:33.730023Z",
          "shell.execute_reply.started": "2022-04-11T02:21:33.634384Z",
          "shell.execute_reply": "2022-04-11T02:21:33.729354Z"
        },
        "trusted": true,
        "id": "LxLGNztDdBQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Peak at the data"
      ],
      "metadata": {
        "id": "zEFrbL0TdBQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#peek at the images and labels\n",
        "print(plt.imshow(vgg_fn_images[1]), vgg_fn_labels[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:36.335878Z",
          "iopub.execute_input": "2022-04-11T02:21:36.336776Z",
          "iopub.status.idle": "2022-04-11T02:21:36.623531Z",
          "shell.execute_reply.started": "2022-04-11T02:21:36.336736Z",
          "shell.execute_reply": "2022-04-11T02:21:36.621531Z"
        },
        "trusted": true,
        "id": "DLowO3rEdBQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#peek at the images and labels\n",
        "print(plt.imshow(vgg_images[1]), vgg_labels[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:38.011394Z",
          "iopub.execute_input": "2022-04-11T02:21:38.012227Z",
          "iopub.status.idle": "2022-04-11T02:21:38.294363Z",
          "shell.execute_reply.started": "2022-04-11T02:21:38.012185Z",
          "shell.execute_reply": "2022-04-11T02:21:38.292337Z"
        },
        "trusted": true,
        "id": "fc2LF9-HdBQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#peek at the images and labels\n",
        "print(plt.imshow(res_images[1]), res_labels[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:39.699060Z",
          "iopub.execute_input": "2022-04-11T02:21:39.700017Z",
          "iopub.status.idle": "2022-04-11T02:21:39.927608Z",
          "shell.execute_reply.started": "2022-04-11T02:21:39.699950Z",
          "shell.execute_reply": "2022-04-11T02:21:39.926786Z"
        },
        "trusted": true,
        "id": "1GDdGGwQdBQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the Accuracy of the Network with different Style Transferred Images"
      ],
      "metadata": {
        "id": "5wsq5_gJdBQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the performence of the network on differnt style-transferred images:\n",
        "#VGG19, content image as input\n",
        "vgg_loss, vgg_accuracy = test(art_transfer_discrimination_model, vgg_data, cross_entropy_loss)\n",
        "print(f'VGG19 Accuracy with Content Image as Input: {vgg_accuracy}.')\n",
        "\n",
        "#VGG19, noise image as input\n",
        "vgg_fn_loss, vgg_fn_accuracy = test(art_transfer_discrimination_model, vgg_fn_data, cross_entropy_loss)\n",
        "print(f'VGG19 Accuracy with Noise Image as Input: {vgg_fn_accuracy}.')\n",
        "\n",
        "#ResNet50, content image as input\n",
        "res_loss, res_accuracy = test(art_transfer_discrimination_model, res_data, cross_entropy_loss)\n",
        "print(f'Res Accuracy with Content Image as Input: {res_accuracy}.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-11T02:21:53.454385Z",
          "iopub.execute_input": "2022-04-11T02:21:53.454646Z",
          "iopub.status.idle": "2022-04-11T02:21:54.199759Z",
          "shell.execute_reply.started": "2022-04-11T02:21:53.454618Z",
          "shell.execute_reply": "2022-04-11T02:21:54.199083Z"
        },
        "trusted": true,
        "id": "_HjHKKChdBQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}